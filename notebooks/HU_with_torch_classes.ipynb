{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snn_hybrid/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Callable\n",
    "from enum import Enum\n",
    "import tonic\n",
    "from torch import optim\n",
    "\n",
    "import tonic.transforms as tonic_transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(torch.nn.Module):\n",
    "    def __init__(self, window_size) -> None:\n",
    "        super(Sampler, self).__init__()\n",
    "        self.window_size = window_size\n",
    "\n",
    "# class LearnableSampler(Sampler):\n",
    "#     def __init__(self, window_size: int) -> None:\n",
    "#         super(LearnableSampler, self).__init__(window_size=window_size)\n",
    "#         self.linear = torch.nn.Linear(1, self.window_size)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = x.unsqueeze(-1)\n",
    "#         x = self.linear(x)\n",
    "#         return x\n",
    "\n",
    "class HU(torch.nn.Module):\n",
    "    def __init__(self, window_size: int, non_linear: torch.nn.Module = None) -> None:\n",
    "        super(HU, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.window_set = None\n",
    "        self.window_conv = None\n",
    "        self.sampler = None\n",
    "        self.non_linear = non_linear\n",
    "        self.precision_convert = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.window_set is not None:\n",
    "            x = self.window_set(x)\n",
    "        if self.window_conv is not None:\n",
    "            x = self.window_conv(x)\n",
    "        if self.sampler is not None:\n",
    "            x = self.sampler(x)\n",
    "        if self.non_linear is not None:\n",
    "            x = self.non_linear(x)\n",
    "        if self.precision_convert is not None:\n",
    "            x = self.precision_convert(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PrecisionConvert(torch.nn.Module):\n",
    "    def __init__(self, converter: Callable[[torch.Tensor], torch.Tensor]) -> None:\n",
    "        super(PrecisionConvert, self).__init__()\n",
    "        self.converter = converter\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.converter(x)\n",
    "        return x\n",
    "\n",
    "class A2SPrecisionConvert(PrecisionConvert):\n",
    "    def __init__(self, converter: Callable[[torch.Tensor], torch.Tensor]) -> None:\n",
    "        super(A2SPrecisionConvert, self).__init__(converter=converter)\n",
    "\n",
    "\n",
    "class A2SHU(HU):\n",
    "    def __init__(self, window_size: int, converter: Callable[[torch.Tensor], torch.Tensor],\n",
    "                 non_linear: torch.nn.Module = None) -> None:\n",
    "        super(A2SHU, self).__init__(window_size, non_linear)\n",
    "        self.precision_convert = A2SPrecisionConvert(converter=converter)\n",
    "\n",
    "    def check(self):\n",
    "        assert (self.window_set is None and self.window_conv is None and\n",
    "                self.sampler is not None and self.precision_convert is not None)\n",
    "\n",
    "class A2SLearnableCoding(A2SHU):\n",
    "    def __init__(self, window_size: int, converter: Callable[[torch.Tensor], torch.Tensor],\n",
    "                 non_linear: torch.nn.Module = None) -> None:\n",
    "        super(A2SLearnableCoding, self).__init__(\n",
    "            window_size, converter, non_linear)\n",
    "        self.sampler = LearnableSampler(window_size=self.window_size)\n",
    "        self.check()\n",
    "\n",
    "class HardUpdateAfterSpike(torch.nn.Module):\n",
    "    def __init__(self, value: float) -> None:\n",
    "        super(HardUpdateAfterSpike, self).__init__()\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self, x: torch.Tensor, spike: torch.Tensor) -> torch.Tensor:\n",
    "        out = spike * self.value + (1 - spike) * x\n",
    "        return out\n",
    "\n",
    "class Rectangle(torch.autograd.Function):\n",
    "    window_size = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, v3: torch.Tensor, v_th) -> torch.Tensor:\n",
    "        ctx.save_for_backward(v3, torch.as_tensor(v_th, device=v3.device))\n",
    "        out = (v3 > v_th).float()\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        v3, v_th = ctx.saved_tensors\n",
    "        mask = torch.abs(v3 - v_th) < Rectangle.window_size / 2\n",
    "        return grad_output * mask.float() * 1 / Rectangle.window_size, None\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g: torch._C.Graph, input: torch._C.Value, v_th0: float) -> torch._C.Value:\n",
    "        return g.op(\"snn::RectangleFire\", input, v_th0_f=v_th0)\n",
    "\n",
    "\n",
    "class Accumulate(torch.nn.Module):\n",
    "    def __init__(self, v_init) -> None:\n",
    "        super(Accumulate, self).__init__()\n",
    "        self.v_init = v_init\n",
    "\n",
    "    def forward(self, u_in, v=None) -> torch.Tensor:\n",
    "        if v is None:\n",
    "            v = torch.full_like(u_in, self.v_init)\n",
    "        return u_in + v\n",
    "\n",
    "class FireWithConstantThreshold(torch.nn.Module):\n",
    "    def __init__(self, surrogate_function, v_th) -> None:\n",
    "        super(FireWithConstantThreshold, self).__init__()\n",
    "        self.surrogate_function = surrogate_function\n",
    "        self.v_th = v_th\n",
    "\n",
    "    def forward(self, v) -> torch.Tensor:\n",
    "        spike = self.surrogate_function.apply(v, self.v_th)\n",
    "        return spike\n",
    "\n",
    "class IF(torch.nn.Module):\n",
    "    '''Integrate-and-Fire\n",
    "    '''\n",
    "    def __init__(self, v_th, v_reset, v_init=None, window_size=1):\n",
    "        super(IF, self).__init__()\n",
    "        self.reset = HardUpdateAfterSpike(value=v_reset)\n",
    "        self.accumulate = Accumulate(\n",
    "            v_init=self.reset.value if v_init is None else v_init)\n",
    "        Rectangle.window_size = window_size\n",
    "        self.fire = FireWithConstantThreshold(\n",
    "            surrogate_function=Rectangle, v_th=v_th)\n",
    "\n",
    "    def forward(self, u_in: torch.Tensor, v: torch.Tensor = None):\n",
    "        print(\"[IF] input u_in shape:\", u_in.shape)\n",
    "        if v is not None:\n",
    "            print(\"[IF] input v shape:\", v.shape)\n",
    "\n",
    "        v_update = self.accumulate(u_in, v)\n",
    "        print(\"[IF] v_update shape:\", v_update.shape)\n",
    "\n",
    "        spike = self.fire(v_update)\n",
    "        print(\"[IF] spike shape:\", spike.shape)\n",
    "\n",
    "        v = self.reset(v_update, spike)\n",
    "        print(\"[IF] new v shape:\", v.shape)\n",
    "\n",
    "        return spike, v\n",
    "\n",
    "\n",
    "class Leaky(torch.nn.Module):\n",
    "    def __init__(self, alpha, beta, adpt_en=True):\n",
    "        super(Leaky, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        assert alpha <= 1\n",
    "        self.adpt_en = adpt_en\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.adpt_en:\n",
    "            out = self.alpha * x + self.beta\n",
    "        else:\n",
    "            out = x + self.beta\n",
    "        return out\n",
    "\n",
    "class LIF(torch.nn.Module):\n",
    "    '''Leaky-Integrate-and-Fire\n",
    "    Args:\n",
    "        if_node.reset.value = v_reset\n",
    "        if_node.accumulate.v_init = v_init\n",
    "        if_node.fire.v_th = v_th\n",
    "        if_node.fire.surrogate_function: Rectangle\n",
    "        v_leaky.alpha = v_leaky_alpha\n",
    "        v_leaky.beta = v_leaky_beta\n",
    "        v_leaky.adpt_en = v_leaky_adpt_en\n",
    "        window_size: Rectangle, default = 1\n",
    "    '''\n",
    "    def __init__(self, v_th, v_leaky_alpha, v_leaky_beta, v_reset=0, v_leaky_adpt_en=False, v_init=None, window_size=1):\n",
    "        super(LIF, self).__init__()\n",
    "        self.if_node = IF(v_th=v_th, v_reset=v_reset,\n",
    "                          v_init=v_init, window_size=window_size)\n",
    "        self.v_leaky = Leaky(alpha=v_leaky_alpha,\n",
    "                             beta=v_leaky_beta, adpt_en=v_leaky_adpt_en)\n",
    "\n",
    "    def forward(self, u_in: torch.Tensor, v=None):\n",
    "        spike, v = self.if_node(u_in, v)\n",
    "        v = self.v_leaky(v)\n",
    "        return spike, v\n",
    "\n",
    "\n",
    "# class OutputRateCoding(torch.nn.Module):\n",
    "#     def __init__(self, dim=0) -> None:\n",
    "#         super().__init__()\n",
    "#         self.dim = dim\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         x = torch.stack(x, dim=0)\n",
    "#         return x.mean(dim=self.dim)\n",
    "\n",
    "class OutputRateCoding(nn.Module):\n",
    "    def __init__(self, dim=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape = (T,B,10)\n",
    "        return x.mean(dim=self.dim)  # => (B,10)\n",
    "\n",
    "# class LearnableSampler(Sampler):\n",
    "#     def __init__(self, window_size: int) -> None:\n",
    "#         super(LearnableSampler, self).__init__(window_size=window_size)\n",
    "#         self.linear = torch.nn.Linear(1, self.window_size)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = x.unsqueeze(-1)\n",
    "#         x = self.linear(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "class LearnableSampler(Sampler):\n",
    "    def __init__(self, window_size: int) -> None:\n",
    "        super().__init__(window_size=window_size)\n",
    "        # Suppose we want to keep output dimension = 576\n",
    "        self.linear = torch.nn.Linear(576, 576)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is already (T, B, 576). Let's flatten T*B dimension to feed linear\n",
    "        T, B, F = x.shape    # F=576\n",
    "        x = x.view(T*B, F)   # => (T*B, 576)\n",
    "        x = self.linear(x)   # => (T*B, 576)\n",
    "        x = x.view(T, B, F)  # => (T, B, 576)\n",
    "        return x\n",
    "\n",
    "class InputMode(Enum):\n",
    "    STATIC = 'static'\n",
    "    SEQUENTIAL = 'sequential'\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, time_interval: int, mode: InputMode) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.time_interval = time_interval\n",
    "        self.mode = mode\n",
    "\n",
    "    def multi_step_forward(self, x, *args):\n",
    "        outputs = []\n",
    "        if self.mode == InputMode.STATIC:\n",
    "            for i in range(self.time_interval):\n",
    "                output, *args = self.forward(x, *args)\n",
    "                outputs.append(output)\n",
    "        elif self.mode == InputMode.SEQUENTIAL:\n",
    "            for i in range(self.time_interval):\n",
    "                output, *args = self.forward(x[i], *args)\n",
    "                outputs.append(output)\n",
    "        else:\n",
    "            raise ValueError('Unsupported input mode')\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class A2SModel(torch.nn.Module):\n",
    "    def __init__(self, T) -> None:\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.ann: torch.nn.Module = None\n",
    "        self.a2shu: A2SHU = None\n",
    "        self.snn: Model = None\n",
    "        self.encode: torch.nn.Module = None\n",
    "\n",
    "    def reshape(self, x: torch.Tensor):\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        x = self.ann(x)\n",
    "        x = self.a2shu(x)  # [N, C, H, W] -> [N, C, H, W, T]\n",
    "        x = self.reshape(x)  # [N, C, H, W, T] -> [T, ...]\n",
    "        x = self.snn.multi_step_forward(x, *args)\n",
    "        return self.encode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, use_conv=False):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        \n",
    "        if use_conv:\n",
    "            self.ann_layer = nn.Conv2d(in_features, out_features, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.ann_layer = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        self.snn_layer = LIF(v_th=0.1, v_leaky_alpha=0.5, v_leaky_beta=0, v_reset=0)\n",
    "\n",
    "    def forward(self, x, v=None):\n",
    "        ann_out = self.ann_layer(x)\n",
    "        spike_out, v = self.snn_layer(ann_out, v)  # SNN processes ANN output\n",
    "        return spike_out, v\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_hybrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
